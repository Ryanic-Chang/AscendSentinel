import logging
import streamlit as st
from typing import Optional, List, Dict
import dashscope
import os
import pandas as pd
from io import StringIO
import json
import warnings

# Suppress NumPy warnings
warnings.filterwarnings("ignore", category=UserWarning, module="numpy.core.getlimits")

# æœ¬åœ°Qwenä¾èµ–
import mindspore as ms
from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer
from mindnlp.transformers import TextIteratorStreamer
from threading import Thread
from mindspore import ops  # æ·»åŠ å¯¼å…¥ops

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# è®¾ç½®é˜¿é‡Œäº‘DashScope APIå¯†é’¥ï¼ˆè¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…å¯†é’¥ï¼‰
os.environ["DASHSCOPE_API_KEY"] = "sk-a1e20166519143fdabfc03d4b38ba595"  # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä»ç¯å¢ƒå˜é‡æˆ–å®‰å…¨æ–¹å¼è·å–

class QwenAnalyzer:
    """ä½¿ç”¨é˜¿é‡Œäº‘Qwenæ¨¡å‹çš„å¤šä»»åŠ¡åˆ†æå™¨ï¼ˆäº‘ç«¯ï¼‰"""
    def __init__(self, model: str = "qwen-turbo"):  # å¯ä»¥é€‰æ‹©qwen-turbo, qwen-plus ç­‰
        self.model = model
        dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")
        if not dashscope.api_key:
            raise ValueError("DASHSCOPE_API_KEY æœªè®¾ç½®")

    def analyze(self, text: str, task: str) -> Optional[str]:
        """é€šç”¨åˆ†ææ–¹æ³•ï¼Œæ ¹æ®ä»»åŠ¡æ„å»ºæç¤ºå¹¶è°ƒç”¨Qwen API"""
        prompts = {
            "summary": f"ä¸ºä¸‹é¢çš„æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼š\n{text}",
            "sentiment": f"åˆ†æä¸‹é¢çš„æ–‡æœ¬æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€ä¸­æ€§ã€æ¶ˆæï¼‰ï¼š\n{text}",
            "keywords": f"ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æŠ½å–å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š\n{text}",
            "intent": f"åˆ†ç±»ä¸‹é¢çš„æ–‡æœ¬æ„å›¾ï¼ˆæŠ•è¯‰ã€èµæ‰¬ã€å»ºè®®ã€å…¶ä»–ï¼‰ï¼š\n{text}",
            "profile": f"åŸºäºå¤šæ¡è¯„è®ºç”Ÿæˆç”¨æˆ·ç”»åƒï¼ˆå…´è¶£ã€æƒ…æ„Ÿå€¾å‘ã€æ€»ç»“ï¼‰ï¼š\n{text}"  # å¯¹äºç”¨æˆ·ç”»åƒï¼Œè¾“å…¥å¤šæ¡è¯„è®ºæ‹¼æ¥
        }
        prompt = prompts.get(task, f"å¤„ç†ä¸‹é¢çš„æ–‡æœ¬ï¼š\n{text}")
        try:
            response = dashscope.Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=200,  # æ§åˆ¶è¾“å‡ºé•¿åº¦
                temperature=0.7,  # è°ƒæ•´åˆ›é€ æ€§
                top_p=1.0
            )
            if response.status_code == 200:
                output = response.output.text.strip()
                logging.info(f"{task} ç»“æœ: {output}")
                return output
            else:
                logging.error(f"API è°ƒç”¨å¤±è´¥: {response.message}")
                return None
        except Exception as e:
            logging.error(f"{task} æ¨ç†å¤±è´¥: {e}")
            return None

class LocalQwenAnalyzer:
    """ä½¿ç”¨æœ¬åœ°Qwenæ¨¡å‹çš„å¤šä»»åŠ¡åˆ†æå™¨"""
    def __init__(self, model_path: str = "Qwen/Qwen1.5-0.5B-Chat"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, dtype=ms.float16)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, dtype=ms.float16)
        self.system_prompt = "You are an expert in text analysis."

    def analyze(self, text: str, task: str) -> Optional[str]:
        prompts = {
            "summary": f"ä¸ºä¸‹é¢çš„æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼š\n{text}",
            "sentiment": f"åˆ†æä¸‹é¢çš„æ–‡æœ¬æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€ä¸­æ€§ã€æ¶ˆæï¼‰ï¼š\n{text}",
            "keywords": f"ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æŠ½å–å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š\n{text}",
            "intent": f"åˆ†ç±»ä¸‹é¢çš„æ–‡æœ¬æ„å›¾ï¼ˆæŠ•è¯‰ã€èµæ‰¬ã€å»ºè®®ã€å…¶ä»–ï¼‰ï¼š\n{text}",
            "profile": f"åŸºäºå¤šæ¡è¯„è®ºç”Ÿæˆç”¨æˆ·ç”»åƒï¼ˆå…´è¶£ã€æƒ…æ„Ÿå€¾å‘ã€æ€»ç»“ï¼‰ï¼š\n{text}"
        }
        user_msg = prompts.get(task, f"å¤„ç†ä¸‹é¢çš„æ–‡æœ¬ï¼š\n{text}")
        messages = [{'role': 'system', 'content': self.system_prompt}, {'role': 'user', 'content': user_msg}]
        
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="ms",
            tokenize=True
        )
        
        # åˆ›å»ºattention_maskæ¥é¿å…è­¦å‘Š
        attention_mask = ops.ones(input_ids.shape, dtype=ms.int64)
        
        streamer = TextIteratorStreamer(self.tokenizer, timeout=300, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = dict(
            input_ids=input_ids,
            attention_mask=attention_mask,  # æ·»åŠ attention_mask
            streamer=streamer,
            max_new_tokens=200,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            num_beams=1,
        )
        t = Thread(target=self.model.generate, kwargs=generate_kwargs)
        t.start()
        partial_message = ""
        for new_token in streamer:
            partial_message += new_token
            if '</s>' in partial_message:
                break
        output = partial_message.strip()
        logging.info(f"{task} ç»“æœ: {output}")
        return output

def parse_user_comments(uploaded_file) -> List[Dict]:
    """è§£æå•ä¸ªç”¨æˆ·TXTï¼šä¸»é¢˜ï¼šXXX æ—¶é—´ï¼šXXX å†…å®¹ï¼šXXX"""
    content = uploaded_file.getvalue().decode("utf-8")
    comments = []
    lines = content.splitlines()
    current = {}
    for line in lines:
        if line.startswith("ä¸»é¢˜ï¼š"):
            if current: comments.append(current)
            current = {"theme": line[3:].strip()}
        elif line.startswith("æ—¶é—´ï¼š"):
            current["time"] = line[3:].strip()
        elif line.startswith("å†…å®¹ï¼š"):
            current["content"] = line[3:].strip()
    if current: comments.append(current)
    return comments

def parse_theme_comments(uploaded_file) -> List[Dict]:
    """è§£æä¸»é¢˜TXTï¼šä¸»é¢˜ï¼šXXX ç”¨æˆ·ï¼šXXX æ—¶é—´ï¼šXXX å†…å®¹ï¼šXXX"""
    content = uploaded_file.getvalue().decode("utf-8")
    comments = []
    lines = content.splitlines()
    current = {}
    for line in lines:
        if line.startswith("ä¸»é¢˜ï¼š"):
            if current: comments.append(current)
            current = {"theme": line[3:].strip()}
        elif line.startswith("ç”¨æˆ·ï¼š"):
            current["user"] = line[3:].strip()
        elif line.startswith("æ—¶é—´ï¼š"):
            current["time"] = line[3:].strip()
        elif line.startswith("å†…å®¹ï¼š"):
            current["content"] = line[3:].strip()
    if current: comments.append(current)
    return comments

# ======================================================================
# Streamlit UI
# ======================================================================

st.set_page_config(page_title="èˆ†æƒ…ç›‘æ§ç³»ç»Ÿ", layout="wide")

st.title("èˆ†æƒ…ç›‘æ§ç³»ç»Ÿï¼ˆQwenÂ·å¢å¼ºç‰ˆï¼‰")
st.sidebar.title("å¯¼èˆª")

analysis_type = st.sidebar.radio("é€‰æ‹©åˆ†æç±»å‹", ("å•ä¸ªç”¨æˆ·åˆ†æ", "ä¸»é¢˜åˆ†æ", "å›¾åƒè§’è‰²åˆ†æ"))

# æ ¹æ®åˆ†æç±»å‹è°ƒæ•´æ¨¡å¼é€‰æ‹©
if analysis_type == "å›¾åƒè§’è‰²åˆ†æ":
    mode = "äº‘ç«¯ (é˜¿é‡Œäº‘Qwen)"  # å¼ºåˆ¶äº‘ç«¯æ¨¡å¼
    st.sidebar.info("ğŸ–¼ï¸ å›¾åƒåˆ†æä»…æ”¯æŒäº‘ç«¯æ¨¡å¼")
else:
    mode = st.sidebar.radio("é€‰æ‹©åˆ†ææ¨¡å¼", ("äº‘ç«¯ (é˜¿é‡Œäº‘Qwen)", "æœ¬åœ°ï¼ˆQwen0.5Bï¼‰"))

# é€‰æ‹©æ¨¡å‹
if mode.startswith("äº‘ç«¯"):
    analyzer = QwenAnalyzer()
else:
    analyzer = LocalQwenAnalyzer() 

# Set MindSpore context with fallback
try:
    ms.set_context(device_target='Ascend', device_id=0, mode=ms.PYNATIVE_MODE)
    st.info("Using Ascend device.")
except Exception as e:
    st.warning(f"Ascend device not available: {e}. Falling back to CPU.")
    ms.set_context(device_target='CPU', mode=ms.PYNATIVE_MODE)

# æ ¹æ®æ¨¡å¼åˆå§‹åŒ–åˆ†æå™¨
if mode == "äº‘ç«¯ (é˜¿é‡Œäº‘Qwen)":
    analyzer = QwenAnalyzer(model="qwen-turbo")
else:
    analyzer = LocalQwenAnalyzer()

if analysis_type == "å•ä¸ªç”¨æˆ·åˆ†æ":
    st.header("å•ä¸ªç”¨æˆ·åˆ†æ")
    username = st.text_input("è¾“å…¥ç”¨æˆ·å")
    uploaded_file = st.file_uploader("ä¸Šä¼ ç”¨æˆ·è¯„è®ºTXTæ–‡ä»¶", type="txt")
    
    if uploaded_file and username:
        comments = parse_user_comments(uploaded_file)
        st.subheader(f"ç”¨æˆ· {username} çš„è¯„è®ºåˆ—è¡¨")
        df = pd.DataFrame(comments)
        st.dataframe(df)
        
        # åˆ†ææ¯æ¡
        st.subheader("æ¯æ¡è¯„è®ºåˆ†æ")
        results = []
        for i, comment in enumerate(comments):
            content = comment['content']
            sentiment = analyzer.analyze(content, "sentiment")
            keywords = analyzer.analyze(content, "keywords")
            intent = analyzer.analyze(content, "intent")
            summary = analyzer.analyze(content, "summary")
            theme = comment.get('theme', 'æœªçŸ¥')
            time = comment.get('time', 'æœªçŸ¥')
            st.markdown(f"**è¯„è®º {i+1} ({theme}, {time})**")
            st.write(f"æƒ…æ„Ÿ: {sentiment}")
            st.write(f"å…³é”®è¯: {keywords}")
            st.write(f"æ„å›¾: {intent}")
            st.write(f"æ‘˜è¦: {summary}")
            results.append({
                "comment_id": i+1,
                "theme": theme,
                "time": time,
                "content": content,
                "sentiment": sentiment,
                "keywords": keywords,
                "intent": intent,
                "summary": summary
            })
        
        # ç”¨æˆ·ç”»åƒ
        all_content = "\n".join([c['content'] for c in comments])
        profile = analyzer.analyze(all_content, "profile")
        st.subheader("ç”¨æˆ·ç”»åƒ")
        st.write(profile)
        results.append({"profile": profile})
        
        # ä¿å­˜æŒ‰é’®
        json_results = json.dumps(results, ensure_ascii=False, indent=4)
        st.download_button(
            label="ä¸‹è½½åˆ†æç»“æœ",
            data=json_results,
            file_name=f"user_{username}_analysis.json",
            mime="application/json"
        )

elif analysis_type == "ä¸»é¢˜åˆ†æ":
    st.header("ä¸»é¢˜åˆ†æ")
    theme_input = st.text_input("è¾“å…¥ä¸»é¢˜")
    uploaded_file = st.file_uploader("ä¸Šä¼ ä¸»é¢˜è¯„è®ºTXTæ–‡ä»¶", type="txt")
    
    if uploaded_file and theme_input:
        comments = parse_theme_comments(uploaded_file)
        st.subheader(f"ä¸»é¢˜ {theme_input} çš„è¯„è®ºåˆ—è¡¨")
        df = pd.DataFrame(comments)
        st.dataframe(df)
        
        # åˆ†ææ¯æ¡
        st.subheader("æ¯æ¡è¯„è®ºåˆ†æ")
        sentiments = []
        results = []
        for i, comment in enumerate(comments):
            content = comment['content']
            sentiment = analyzer.analyze(content, "sentiment")
            keywords = analyzer.analyze(content, "keywords")
            intent = analyzer.analyze(content, "intent")
            summary = analyzer.analyze(content, "summary")
            user = comment.get('user', 'æœªçŸ¥')
            time = comment.get('time', 'æœªçŸ¥')
            st.markdown(f"**è¯„è®º {i+1} ({user}, {time})**")
            st.write(f"æƒ…æ„Ÿ: {sentiment}")
            st.write(f"å…³é”®è¯: {keywords}")
            st.write(f"æ„å›¾: {intent}")
            st.write(f"æ‘˜è¦: {summary}")
            sentiments.append(sentiment)
            results.append({
                "comment_id": i+1,
                "user": user,
                "time": time,
                "content": content,
                "sentiment": sentiment,
                "keywords": keywords,
                "intent": intent,
                "summary": summary
            })
        
        # æ•´ä½“æ€»ç»“
        all_content = "\n".join([c['content'] for c in comments])
        overall_summary = analyzer.analyze(all_content, "summary")
        sentiment_dist = pd.Series(sentiments).value_counts().to_dict()
        st.subheader("æ•´ä½“æ€»ç»“")
        st.write(f"æƒ…æ„Ÿåˆ†å¸ƒ: {sentiment_dist}")
        st.write(f"æ€»ä½“æ‘˜è¦: {overall_summary}")
        results.append({
            "sentiment_distribution": sentiment_dist,
            "overall_summary": overall_summary
        })
        
        # ä¿å­˜æŒ‰é’®
        json_results = json.dumps(results, ensure_ascii=False, indent=4)
        st.download_button(
            label="ä¸‹è½½åˆ†æç»“æœ",
            data=json_results,
            file_name=f"theme_{theme_input}_analysis.json",
            mime="application/json"
        )

# è¿è¡Œæç¤º
if __name__ == "__main__":
    st.write("åº”ç”¨å·²åŠ è½½ã€‚é€‰æ‹©å¯¼èˆªå’Œæ¨¡å¼å¼€å§‹åˆ†æã€‚")

# ======================================================================
# æ–°åŠŸèƒ½ï¼šå›¾åƒè§’è‰²åˆ†æ
# ======================================================================

elif analysis_type == "å›¾åƒè§’è‰²åˆ†æ":

    st.header("å›¾åƒè§’è‰²åˆ†æ")
    uploaded_image = st.file_uploader("ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶", type=["jpg", "jpeg", "png"], help="æ”¯æŒ JPG/PNG æ ¼å¼ï¼Œä¸Šä¼ åè‡ªåŠ¨åˆ†æ")

    if uploaded_image:
        st.image(uploaded_image, caption="ä¸Šä¼ çš„å›¾ç‰‡", width=400)  # æ§åˆ¶å°ºå¯¸ä¸º400px

        # è¯»å–å›¾ç‰‡ä¸º base64
        image_data = uploaded_image.getvalue()
        base64_image = base64.b64encode(image_data).decode("utf-8")
        mime_type = uploaded_image.type  # e.g., image/jpeg

        # Prompt
        prompt = "ä½ æ˜¯ä¸€åæœ‰ç€å¤šå¹´åˆ‘ä¾¦ç»éªŒå’Œæ–­æ¡ˆç»éªŒçš„è­¦å¯Ÿï¼Œä½ å°¤å…¶å¯¹äººç‰©è‚–åƒæ•æ„Ÿï¼Œä½ å¯ä»¥è½»æ¾çš„åˆ¤æ–­å‡ºç”»é¢ä¸­çš„äººçš„å¹´é¾„ã€èŒä¸šã€æ€§æ ¼ç­‰ç­‰å„é¡¹èº«ä»½ä¿¡æ¯ï¼Œç»™å‡ºä½ çš„åˆ†æã€‚"

        # å¤šæ¨¡æ€è°ƒç”¨
        messages = [
            {
                "role": "user",
                "content": [
                    {"image": f"data:{mime_type};base64,{base64_image}"},
                    {"text": prompt}
                ]
            }
        ]

        try:
            with st.spinner("ğŸ” æ­£åœ¨åˆ†æå›¾ç‰‡..."):
                response = dashscope.MultiModalConversation.call(
                    model="qwen-vl-plus",
                    messages=messages
                )
            if response.status_code == 200:
                analysis_result = response.output.choices[0].message.content[0]["text"]
                st.subheader("åˆ†æç»“æœ")
                st.write(analysis_result)

                # ä¸‹è½½æŒ‰é’®
                results = {"analysis": analysis_result}
                st.download_button(
                    "ğŸ“¥ ä¸‹è½½åˆ†æç»“æœ",
                    json.dumps(results, ensure_ascii=False, indent=4),
                    "image_analysis.json",
                    "application/json"
                )
            else:
                st.error(f"API è°ƒç”¨å¤±è´¥: {response.message}")
        except Exception as e:
            st.error(f"è°ƒç”¨å¤±è´¥: {e}")

if __name__ == "__main__":
    st.write("åº”ç”¨å·²åŠ è½½ã€‚é€‰æ‹©å¯¼èˆªå’Œæ¨¡å¼å¼€å§‹åˆ†æã€‚")